## 抢红包系统设计

[TOC]

## 问题

1. 在游戏中，玩家A向全服玩家发送红包，其他玩家可以通过“抢红包”的操作来获得红包中的部分奖励，每个红包会被10个玩家瓜分。

2. 尝试思考，当有1000名玩家几乎同时发起“抢红包”操作时，如何保证每个玩家的操作延时在 10ms以下？

## 系统整体实现思路

### 系统设计原则

在讨论系统如何设计前，要明确一下系统的核心目的：优化并发读写。

对于高并发读写的系统，设计原则有以下几点：

1. 缩短读取路径，即缩短读操作的时间。对于高并发抢红包的系统，缓存是必不可少的。读取缓存时，从磁盘上读取的时间显然远大于读取内存的时间，同时网络操作如RPC等操作也会严重拖慢读取时间，故需要做到尽量减少磁盘、网络IO读取的次数。

2. 尽量不要将运算资源调度给正在内核态进行IO操作的并发任务。正在进行IO操作的线程实际上是无法执行别的任务的，若这个线程获取到运算资源，则相当于浪费了至少两次上下文切换的时间。

3. 并发任务竞争更新数据保证数据一致性的同时，尽量避免锁的应用，或者在读多写少的场景中应用乐观锁。理论上无锁化(lock free)编程的性能是远大于有锁编程的。

5. 尽量确保系统高可用，一个并发任务出现故障的时，要尽量避免影响到其它请求的正常处理。

### 系统架构图

![](http://cdn.lentme.cn/20221230113600.png)

架构解析：

每一个用户都会将抢红包的请求提交到多进程服务器(HTTP服务器)，每一个HTTP服务器进程都共同监听一个服务器端口，且其内部由三大部分组成：

- Acceptor： 负责监听服务器端口，接收用户连接后调度给Worker去处理请求。
- Worker： 负责接收并解析处理用户的请求内容，最终对用户的请求做出响应。
- 本地缓存：用户抢红包时，Worker会优先查询本地缓存，减少IPC访问缓存服务的次数，从而提高系统性能。

### 事件驱动的协程模型

服务器的并发模型基于协程开发，相较于多线程，协程模型有两大优势：

1. 协程调度完全由用户控制，且协程上下文切换在堆中进行，减少了线程频繁的内核切换上下文的开销。

2. 多个协程工作在同一个线程中，且事件驱动调度的协程是非抢占式的，故而可以有效地减少资源竞争的问题，如等待锁、信号量等。

事件驱动的协程的应用需要满足以下两种条件：

1. 业务场景中，耗时的操作主要是IO操作。
2. 非IO逻辑程序中不存在如死循环的操作。

假设有以下两个并发任务：

![](http://cdn.lentme.cn/20221230135211.png)

在事件驱动协程模型中，实际任务执行顺序是这样的：

![](http://cdn.lentme.cn/20221230135851.png)

即两个并发任务都在同一个线程中运行，故在线程的视角内，协程的并发任务都是串行的。

为了能够更好地利用到服务器多核的能力，程序一般都要基于多线程开发。故程序可通过`fork`多个进程，从而有效利用了多核能力。

相交于多线程，多进程的优势如下：

1. Linux采用的是一级调度的方式，即基本调度单位是线程，故在上下文切换开销上，多进程多协程和多线程多协程的效率并不会有太大的差距。

2. 进程之间具有很强的封闭性，一个线程可能会影响到整个进程的稳定性，但一个进程是不会影响到其他进程运行的。

3. 多线程能利用到多核的优势，而多进程能以更小的代价利用到分布式多主机的优势。

### 协程模型与线程池比较

可以验证线程池相较于重复新建线程更为高效，但在实际应用中，线程池往往会因为时间片的耗尽而导致触发上下文的切换。

当一个线程持有抢红包的唯一锁且未完成金额分配时恰好时间片耗尽而切换上下文，最差的情况下会导致线程池内所有线程空转了一圈而消耗了相应的时间。

第二种情况则是假设用户存在一些其他的IO操作或者锁竞争时，则会有一部分包括时间在内的的资源开销，这也是线程池无法避免的地方。

综上，若能够控制当前所执行的任务在合适的时候主动让出CPU运算资源，可能可以更好的去优化高并发的接待能力。

#### QPS验证

在单机四核的Linux上进行QPS验证：

> 基于协程自己开发的`FishWebServer`和基于线程池开发的`TinyWebServer`的QPS比较
> 
> 两种服务器业务逻辑基本相同，并且两种服务器的工作线程数量相等。
> 
> 以下图示中，横轴为每秒并发量，纵轴表示QPS

![QPS比较](http://cdn.lentme.cn/20221225150207.png)

从图中可以看出，当并发量10k时，协程的接待能力甚至是线程池的两倍。

因此选择采用协程的方案来开发服务器，采用协程开发的另外一个好处则是，用户可以使用同步的代码来写异步逻辑程序，这能够极大的简化用户程序逻辑以及提升代码的阅读性。


### 高并发缓存方案

高并发缓存基于以下几点方案开发：

1. 由于缓存是直接在内存上操作的，故采用单线程开发，进一步减少锁的使用。

2. 为保证缓存数据的一致性，锁的应用是必要的。锁的实现方案接下来会针对性讨论分析。

3. 采用基于哈希的数据存储方案，哈希表的读取效率理论上是所有存储方案中最快的，时间复杂度都为O(1)。当出现哈希冲突时，采用拉链法解决，但链表太长也会拉低整个哈希表的读写效率，故而要将过长的链表转换成红黑树进行操作。C++ STL中提供的`unordered_map`正好满足以上的需求。

4. 为实现进程间通信，同时又要降低代码的复杂度，缓存程序基于socket实现的RPC的形式与其他进程进行通信。

5. 抢红包的系统是典型的读多写少的场景，程序在读取缓存时，其请求路径应该要尽量短。在一个进程内，可以维护一个本地的缓存，当出现大量读取的时候，用户可以读取进程内的缓存数据，以减少用户读取数据的时间。

做了在不同并发量下，本地缓存能够为缓存服务挡下多少请求的测试： 

    测试条件： 并发请求都会共同竞争同一个只有10个名额的红包

多次测试后取平均值，得到以下图表：

![](http://cdn.lentme.cn/20230101152431.png)

可以看到，绝大部分的请求都会在本地缓存中结束了用户请求的路径，即红包被抢完后，后续大量的请求都不会访问缓存服务。

当并发量大的时候，实际上能够打到缓存服务的请求只占有极小的部分，其它请求都会在后端读取进程本地缓存的时候直接结束请求路径，具体如下图所示：

![](http://cdn.lentme.cn/20221230230500.png)

### 高并发中数据一致性

在高并发的系统中，为了能够在并发读写时保证数据一致性，往往都会引入锁的概念。

在本次设计的抢红包系统中，锁主要有两个方面：

1. 进程内锁，同一进程内并发任务如何保证数据一致性
2. 进程间锁，不同进程同时读写同一个缓存如何保证数据一致性

根据共享性来分类时，主要有独占锁和共享锁两种。前者不论读写都是线程独占的，而后者允许多个线程同时读而独占写。根据场景实际情况可知写锁都是独占的，但为了提高性能，往往需要确保并发读能够同时进行，故常常设计的自然是共享锁。

共享锁一般是锁写不锁读，但如果能够进一步优化，即读写都不锁，性能肯定能够得到大幅度的提升。在此基础上，可以将锁的分类分成悲观锁、乐观锁两种。前者认为每次更新操作时，都会有另一个并发任务在同时更新，此时会悲观加锁以确保数据一致性。

乐观锁认为，在每次更新操作时，不会有其他并发任务在同时更新，故不会对资源访问之前进行上锁的操作。

在进程内设计锁。同一进程内的多个协程，其本质上是串行且非抢占式的，所以在访问进程内的缓存时不需要加锁便能够保证数据一致性。

不同进程间的锁，即HTTP进程在访问缓存服务时，应如何保证数据一致性，可以如下考虑：

- 方案一：拆红包的逻辑放在缓存服务中，用户每次访问都会拆一次红包。
- 方案二：设计分布式锁，用户拆包后互斥更新红包缓存。
- 方案三：缓存程序中实现预拆包的子线程，用户可以直接向缓存取红包
- 方案四：实现乐观锁，即让缓存支持CAS操作，用户抢到红包后CAS更新缓存。

方案一优点在于节省空间，同时利用了缓存服务单进程自然同步的特点。但空间和效率的优化都是呈反比的，即这个方案对缓存服务的指责增多而违背了单一职责原则的设计。

方案二分布式锁会导致用户的请求需要多走几次IPC操作，如实现分布式自旋锁失败重试的机制会导致处理大量的并发请求需要无效地执行一次甚至多次缓存操作。

方案三是方案一的改版，同样违背了单一职责原则，预拆包的方案可能不是提高性能的最佳方案，但绝对是代码复杂度最高且最不易于维护的方案。

方案四是对方案一和方案二的改进，只是将分布式锁改成CAS操作而仅需一次IPC操作即可实现并发写的功能。CAS适合在读多写少的情况，而这次抢红包系统设计中HTTP后端进程中也有自己的本地缓存，能够挡下大部分的请求，根据上一个小节的测试结果，缓存服务完全可以采用CAS的方式去更新缓存。

综上所述，在已经能够实现读多写少的系统下，方案四即这次抢红包系统所采用的实现方案。需要考虑的问题如下：

1. 缓存CAS如何实现？

2. CAS更新失败需要怎么做？

简单的CAS可以通过在红包的数据结构中设计一个用于CAS比较的字段，用户在每次查询时，都会记录下该字段的值。进行更新操作时，缓存服务会比较用户提交的值和当前缓存的值，若存在不一致，则会放弃更新缓存的操作，并返回失败的状态。

```c++
typedef struct red_paper_t {
    double money;
    long num; /* 红包的剩余名额，同时作为CAS的比较字段 */
} red_paper_t;

// ...

// 缓存端cas更新代码
bool grab_cas(rpc_conn conn, unsigned long id, double cal, int cas_num)
{
    // 判断是否存在缓存
    if(cache.find(id) ==  cache.end()) return false;
    // CAS校验
    red_paper_t *p = &cache[id];
    if(p->num != cas_num) return false;
    // 单线程的缓存服务无需担心修改到一半时有其他请求读写数据
    p->money -= cal;
    p->num--;
    return true;
}
```

至此，不论是业务处理服务端还是缓存服务端，实际上都是无锁化编程(lock-free)，且这样的方案在抢红包的高并发场景中保证数据一致性的同时也能够将性能做到极致的优化。

### 高并发缓存服务交互方案

上文主要讨论了缓存服务程序应如何缓存数据，以及更新数据时如何保证一致性。对于缓存服务与后台业务处理的服务进行交互方式，高并发场景下，需要设计一套IPC通讯的方案，这样便只需要抽象约定交互方式，即可在缓存管理进程中读写数据。

主要有以下两种实现方案：

1. 基于内存映射实现
2. 基于socket实现

基于内存映射的IPC实现可以采用`shm`进行开发，`shm`将数据保存在主存中，理论上在单机上延时是最低的，但也存在着以下几个问题：

1. 只允许单机实现，如果在分布式场景下，需要额外开发一些程序进行辅助，但会导致性能大打折扣。
2. 由于内存映射是由内核管理的，故进程之间无法感知对方的存在或者不存在，需要写类似于tcp心跳包的功能去实现。
3. Linux会在最后一个关联映射内存的进程退出时，通知其回收映射内存，但该进程如果是中途崩溃退出，则会导致`/dev/shm`存在未回收的映射内存(如下图)，目前较快的解决方法则是重启计算机或者`rm /dev/shm/*`

![](./figure/ipcs.png)

故较为合理的设计是基于socket实现的RPC框架方案，下图是`Apache brpc`提供的rpc性能测试结果:

![](http://cdn.lentme.cn/20221225161300.png)

图中百度的brpc有着"一览众山小"的感觉，但后续推出的`rest_rpc`在性能上相较brpc显然更有优势。

> 以下图示横轴是client线程数，纵轴是QPS

![](http://cdn.lentme.cn/20221225162230.png)

故选用`rest_rpc`作为接下来开发的基础rpc库进行开发。

## 业务功能编码设计

### 随机分配红包剩余金额

这是最简单的随机实现方法，玩家每次打开红包都会随机分配红包剩余的金额。

![剩余金额随机分配](http://cdn.lentme.cn/20221216222519.png)

> 剩余金额随机分配很容易导致前几个抢到红包的用户就是收起最佳的，而后续的用户打开红包甚至只能拿到"低保"金额，故需改进

### 二倍均值法

二倍均值法理论上保证了每次拆红包时所获得的金额范围都是相等的，而不会因为拆包的先后顺序而导致随机范围越来越小。

![二倍均值法](http://cdn.lentme.cn/20221216224607.png)

#### 二倍均值法实现关键代码

```c++
// 随机数生成引擎
random_device rd{};
mt19937 engine{rd()};
// 随机数分布范围
uniform_real_distribution<double> dist{0.0, 1.0};
// 取随机数
double random_number_between_0_and_1 = dist(engine);
// 保底0.01
double cal = 0.01 + ceil(100 * (random_number_between_0_and_1 * (2 * (remain_money / remain_num) - 0.01))) / 100;
```

### 设计完整系统程序流程

![](./figure/coroutine.jpg)

> 说明： 以上图示中均无线程操作，RPC进程和HTTP进程都是单线程的。
>
> 采用的是fork出多个进程的方法以达到利用多核的目的。

编码实现(抢红包): 

> 具体实现已经上传至github: https://github.com/Rcklos/shiyue_homeworks/blob/master/red_paper_exam/code/test/test_http_server.cpp

```c++
servlet_dispatch->SetGlobServlet("*", [&](HttpRequest::Ptr req, HttpResponse::Ptr rsp, HttpSession::Ptr session){
    // 将请求路径path转换成红包id
    std::string path = req->GetPath();
    path.erase(0, 1);
    unsigned long id = std::stoul(path);

    // 抢红包实现，参考流程图[figure/coroutine.jpg]
    snprintf(str, 200, "red paper is over!\n");
    red_paper_t *p = nullptr;
    // 查询本地缓存
    std::unordered_map<unsigned long, red_paper_t>::iterator it;
    if((it = cache.find(id)) != cache.end()) {
        p = &cache[id];
    }
    // 如果缓存不存在或者还有剩余名额，则开始抢红包
    if(p == nullptr || p->num > 0) {
        bool cas_result = false;
        unsigned int count = 5;
        // 有限次数重试抢红包操作
        while(!cas_result && --count) {
            red_paper_t rp = cli.call<red_paper_t>("select", id);
            if(rp.money == -1) {
                if((it = cache.find(id)) != cache.end()) cache.erase(it);
                p = nullptr;
                rsp->SetBody("red paper not found!!!\n");
                return 0;
            }
            // 更新本地缓存
            if((it = cache.find(id)) != cache.end()) {
                p = &cache[id];
                p->money = rp.money;
                p->num = rp.num;      // 这个num字段也是CAS比较字段
            }
            else cache[id] = { rp.money, rp.num };
            p = &cache[id];
            if(rp.num > 0) {
                // 拆包
                double cal = std::calculate_one_paper_by_double_mean(rp.money, rp.num);
                // CAS更新抢红包结果
                if((cas_result = cli.call<bool>("grab_cas", id, cal, rp.num))) {
                    snprintf(str, 200, "grab money: %.2lf\n", cal);
                    // 打印调试
                    std::cout << str;
                }
                // 更新本地缓存
                p->money = rp.money - cal;
                p->num = rp.num - 1;
            }
        }
    }
    // 响应结果
    rsp->SetBody(str);
    return 0;
});
```

测试结果：

> 发红包的url: http://localhost:9000/insert
> 
> 响应结果是红包id，如果插入失败会响应0
> 
> 抢红包的url: http://localhost:9000/[红包id]

![](http://cdn.lentme.cn/20221231022928.png)

经过多次测试，对于1k的并发请求，平均耗时仅有1ms左右，最长耗时仅2.6ms左右。

### 数据持久化

综合以上的优化方案，数据持久化的拓展空间其实很大，高并发抢红包的情况下，实际上只访问到本地缓存的请求数远大于访问缓存服务的请求数，而最终能够将抢红包记录提交到数据库的请求数极少，故用户抢到红包后可以直接向数据库新增抢红包的记录。

数据库新增提交的方案主要有同步和异步两种，相较于同步，异步的形式更能够减少用户的操作时延。

实际设计时也需要考虑到数据库提交失败时应该要有重试的步骤，以及在给用户反馈结果之前应该要提前将操作记录提交到系统日志中，当异步提交的程序中断时，也能够通过日志进行恢复操作。

## 总结

本次抢红包的系统设计方案主要有以下几个亮点：

1. 基于高性能的协程方案实现HTTP服务器。
2. 应用进程间、进程内双层缓存策略，对大量的请求进行了过滤。
3. 设计无锁化编程系统，降低程序代码的复杂度，同时在保证数据一致性的同时一定程度上提高了系统可用性。
4. 应用socket实现的RPC框架作为进程间IPC通讯方案。

存在的问题主要是未编码实现数据持久化的部分以及用于系统监控的日志等细节部分。题意本身的需求及优化思路都已经基本实现了。

### 附

整套的程序以及文档均提交在Github: 

https://github.com/Rcklos/shiyue_homeworks/tree/master/red_paper_exam

程序依赖于自己的一套协程web开发库，故需在编译该抢红包系统前需安装一下：

```bash
git clone https://github.com/Rcklos/FishWebServer.git
cd FishWebServer/lib/libfish/
chmod +x ./install.sh
./install.sh
```

#### 抢红包系统安装运行

程序运行环境：

```
4C8G:
OS: Arch Linux
Kernel: 6.0.12-arch1-1
```

安装系统

```bash
https://github.com/Rcklos/shiyue_homeworks.git
# 进入抢红包系统代码目录
cd shiyue_homeworks/red_paper_exam/code
# 运行安装脚本
chmod +x install.sh
./install.sh
```

运行系统

```bash
# 在shiyue_homeworks/red_paper_exam/code目录下
# 使用8个进程，每个进程都有20个协程的形式运行
# (具体可根据当前运行的机器调整参数)
./build/bin/http_server 8 20
```

系统压测

采用的http压测工具: `wrk2`，不建议使用ab(apache bench)

```bash
# 插入新红包
# (硬编码初始化为10个名额，总金额是100.0元)
curl http://localhost:9000/insert
# 响应结果是红包id
# 假设红包id为: 1672562791378316

# 压测(连接数10, 每秒请求1500(有偏差))
wrk2 -t6 -c10 -R1500 -d1 http://localhost:9000/1672562791378316

Running 1s test @ http://localhost:9000/1672562791378316
  6 threads and 10 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.09ms  391.46us   2.75ms   66.27%
    Req/Sec       -nan      -nan   0.00      0.00%
  1506 requests in 1.00s, 123.52KB read
Requests/sec:   1504.82
Transfer/sec:    123.43KB
```
